{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a47009b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NM', 'EL', 'MD', 'SV']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "class dls:\n",
    "    vocab = ['NM', 'EL', 'MD', 'SV']\n",
    "dls.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e431e04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['arch', 'hyperparams', 'total params', 'fold_no', 'train loss',\n",
       "       'valid loss', 'accuracy', 'roc_score', 'time', 'test_acc', 'test_f1_EL',\n",
       "       'test_f1_MD', 'test_f1_NM', 'test_f1_SV', 'test_auc_scr',\n",
       "       'train_splits', 'test_splits', 'prob_scores', 'predictions', 'y_test',\n",
       "       'test_f1_weighted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = 'optuna_bests.pkl'\n",
    "with open(save_path, 'rb') as f:\n",
    "  results_rand, results_subj = pickle.load(f)\n",
    "\n",
    "results_rand.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933100c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_score</th>\n",
       "      <td>0.986</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_acc</th>\n",
       "      <td>0.908</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_weighted</th>\n",
       "      <td>0.907</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_NM</th>\n",
       "      <td>0.942</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_EL</th>\n",
       "      <td>0.844</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_MD</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_SV</th>\n",
       "      <td>0.966</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   mean    std\n",
       "roc_score         0.986  0.007\n",
       "test_acc          0.908  0.023\n",
       "test_f1_weighted  0.907  0.023\n",
       "test_f1_NM        0.942  0.027\n",
       "test_f1_EL        0.844  0.060\n",
       "test_f1_MD        0.850  0.030\n",
       "test_f1_SV        0.966  0.011"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'mean':results_rand[['roc_score','test_acc', 'test_f1_weighted',\n",
    "              'test_f1_'+dls.vocab[0], 'test_f1_'+dls.vocab[1],\n",
    "              'test_f1_'+dls.vocab[2], 'test_f1_'+dls.vocab[3]]].mean(axis=0).round(3),\n",
    "             'std':results_rand[['roc_score','test_acc', 'test_f1_weighted',\n",
    "              'test_f1_'+dls.vocab[0], 'test_f1_'+dls.vocab[1],\n",
    "              'test_f1_'+dls.vocab[2], 'test_f1_'+dls.vocab[3]]].std(axis=0).round(3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0811f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_score</th>\n",
       "      <td>0.933</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_acc</th>\n",
       "      <td>0.762</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_weighted</th>\n",
       "      <td>0.764</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_NM</th>\n",
       "      <td>0.873</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_EL</th>\n",
       "      <td>0.619</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_MD</th>\n",
       "      <td>0.634</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1_SV</th>\n",
       "      <td>0.871</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   mean    std\n",
       "roc_score         0.933  0.026\n",
       "test_acc          0.762  0.057\n",
       "test_f1_weighted  0.764  0.058\n",
       "test_f1_NM        0.873  0.070\n",
       "test_f1_EL        0.619  0.085\n",
       "test_f1_MD        0.634  0.123\n",
       "test_f1_SV        0.871  0.065"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'mean':results_subj[['roc_score','test_acc', 'test_f1_weighted',\n",
    "              'test_f1_'+dls.vocab[0], 'test_f1_'+dls.vocab[1],\n",
    "              'test_f1_'+dls.vocab[2], 'test_f1_'+dls.vocab[3]]].mean(axis=0).round(3),\n",
    "             'std':results_subj[['roc_score','test_acc', 'test_f1_weighted',\n",
    "              'test_f1_'+dls.vocab[0], 'test_f1_'+dls.vocab[1],\n",
    "              'test_f1_'+dls.vocab[2], 'test_f1_'+dls.vocab[3]]].std(axis=0).round(3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf20ad88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_auc_scr</th>\n",
       "      <th>roc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.986346</td>\n",
       "      <td>0.986346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.979788</td>\n",
       "      <td>0.979788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.985482</td>\n",
       "      <td>0.985482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.996293</td>\n",
       "      <td>0.996293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.980752</td>\n",
       "      <td>0.980752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_auc_scr  roc_score\n",
       "0      0.986346   0.986346\n",
       "1      0.979788   0.979788\n",
       "2      0.985482   0.985482\n",
       "3      0.996293   0.996293\n",
       "4      0.980752   0.980752"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_rand[['test_auc_scr', 'roc_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c325ba77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_acc': 0.9075862068965517,\n",
       " 'test_f1_weighted': 0.906904046461367,\n",
       " 'test_f1': array([0.84415584, 0.8502994 , 0.94182825, 0.96644295])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.array(results_rand['predictions'].values.tolist()).reshape(-1)\n",
    "y_test = np.array(results_rand['y_test'].values.tolist()).reshape(-1)\n",
    "\n",
    "metrics = {}\n",
    "metrics['test_acc'] = accuracy_score(y_test, predictions)\n",
    "metrics['test_f1_weighted'] = f1_score(y_test, predictions, labels=[0,1,2,3], average='weighted')\n",
    "metrics['test_f1'] = f1_score(y_test, predictions, labels=[0,1,2,3], average=None)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cd4b9d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_acc': 0.7655172413793103,\n",
       " 'test_f1_weighted': 0.7676498204680301,\n",
       " 'test_f1': array([0.62666667, 0.64109589, 0.86908078, 0.88262911])}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "[predictions.extend(i) for i in results_subj['predictions'].values];\n",
    "predictions = np.array(predictions)\n",
    "y_test = []\n",
    "[y_test.extend(i) for i in results_subj['y_test'].values];\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "metrics = {}\n",
    "metrics['test_acc'] = accuracy_score(y_test, predictions)\n",
    "metrics['test_f1_weighted'] = f1_score(y_test, predictions, labels=[0,1,2,3], average='weighted')\n",
    "metrics['test_f1'] = f1_score(y_test, predictions, labels=[0,1,2,3], average=None)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10fc8786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random split value counts\n",
      "fold 0:  Counter({3: 44, 2: 36, 1: 34, 0: 31})\n",
      "fold 1:  Counter({3: 44, 2: 36, 1: 35, 0: 30})\n",
      "fold 2:  Counter({3: 44, 2: 35, 1: 35, 0: 31})\n",
      "fold 3:  Counter({3: 45, 2: 35, 1: 34, 0: 31})\n",
      "fold 4:  Counter({3: 45, 2: 35, 1: 34, 0: 31})\n",
      "subject based split value counts\n",
      "fold 0:  Counter({3: 52, 2: 41, 1: 38, 0: 33})\n",
      "fold 1:  Counter({2: 38, 3: 37, 1: 36, 0: 25})\n",
      "fold 2:  Counter({1: 42, 2: 38, 3: 32, 0: 31})\n",
      "fold 3:  Counter({3: 45, 2: 31, 0: 31, 1: 27})\n",
      "fold 4:  Counter({3: 56, 0: 34, 2: 29, 1: 29})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('random split value counts')\n",
    "for i in range(5):\n",
    "    print(f'fold {i}: ', Counter(results_rand['y_test'].values[i]))\n",
    "\n",
    "print('subject based split value counts')\n",
    "for i in range(5):\n",
    "    print(f'fold {i}: ', Counter(results_subj['y_test'].values[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b85f3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def compute_additional_metrics(y_true, y_pred, y_prob=None, average='weighted'):\n",
    "    \"\"\"\n",
    "    Compute AUC, PPV (Precision), and NPV for multiclass classification results.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: array-like of shape (n_samples,) - Ground truth labels.\n",
    "    - y_pred: array-like of shape (n_samples,) - Predicted labels.\n",
    "    - y_prob: array-like of shape (n_samples, n_classes) - Predicted probabilities for each class.\n",
    "              Required for AUC calculation.\n",
    "    - average: str - Averaging method for AUC and PPV (e.g., 'weighted', 'macro').\n",
    "\n",
    "    Returns:\n",
    "    - metrics: dict - Dictionary containing AUC, PPV, and NPV.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # AUC for multiclass (One-vs-Rest)\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_prob, multi_class='ovr', average=average)\n",
    "        except ValueError:\n",
    "            auc = None  # Could not compute due to class imbalance or missing classes\n",
    "        metrics['AUC'] = auc\n",
    "    else:\n",
    "        metrics['AUC'] = None\n",
    "\n",
    "    # PPV (Precision)\n",
    "    ppv = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    metrics['PPV'] = ppv\n",
    "\n",
    "    # NPV: calculated per class and averaged (weighted by class support)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    npv_list = []\n",
    "    weights = []\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        tn = np.sum(cm) - (tp + fp + fn)\n",
    "        npv_class = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "        npv_list.append(npv_class)\n",
    "        weights.append(np.sum(cm[i, :]))  # Support (number of true samples in class i)\n",
    "\n",
    "    npv = np.average(npv_list, weights=weights)\n",
    "    metrics['NPV'] = npv\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f472b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random split\n",
      "{'AUC': 0.9872667723169826, 'PPV': 0.9067835045440245, 'NPV': 0.9715969041806334}\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "[predictions.extend(i) for i in results_rand['predictions'].values];\n",
    "predictions = np.array(predictions)\n",
    "y_test = []\n",
    "[y_test.extend(i) for i in results_rand['y_test'].values];\n",
    "y_test = np.array(y_test)\n",
    "prob_scores = []\n",
    "[prob_scores.extend(i) for i in results_rand['prob_scores'].values];\n",
    "prob_scores = np.array(prob_scores)\n",
    "metrics = compute_additional_metrics(y_test, predictions, prob_scores)\n",
    "print('random split')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7494a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject based split\n",
      "{'AUC': 0.9383254632754701, 'PPV': 0.7720312220575497, 'NPV': 0.9240367834900114}\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "[predictions.extend(i) for i in results_subj['predictions'].values];\n",
    "predictions = np.array(predictions)\n",
    "y_test = []\n",
    "[y_test.extend(i) for i in results_subj['y_test'].values];\n",
    "y_test = np.array(y_test)\n",
    "prob_scores = []\n",
    "[prob_scores.extend(i) for i in results_subj['prob_scores'].values];\n",
    "prob_scores = np.array(prob_scores)\n",
    "metrics = compute_additional_metrics(y_test, predictions, prob_scores)\n",
    "print('subject based split')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad27ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kneeoa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
